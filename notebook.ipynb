{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from clean_data import create_new_features, get_plate_info, get_government_series\n",
    "from data.supplemental_english import GOVERNMENT_CODES, REGION_CODES\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the datasets...\n",
      "Train data shape: (51635, 4)\n",
      "Test data shape: (7695, 4)\n",
      "Combined data shape: (59330, 5)\n"
     ]
    }
   ],
   "source": [
    "# Read the train and test datasets\n",
    "print(\"Read the datasets...\")\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "print(f\"Train data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "\n",
    "# Add a column 'train' to determine if the row is from the train or test set, it is easy to process both of the train and test sets \n",
    "# at the same time\n",
    "train['train'] = 1\n",
    "test['train'] = 0\n",
    "df = pd.concat([train, test], ignore_index=True)\n",
    "print(f\"Combined data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the plate...\n",
      "Extracted: government_plate, region, first_three, last_three, full_number, series\n"
     ]
    }
   ],
   "source": [
    "# All functions to extract the information from the plate\n",
    "def is_all_latin(s):\n",
    "    return bool(re.fullmatch(r'[A-Za-z0-9]+', s))\n",
    "\n",
    "def get_government_series():\n",
    "    gov_series = set()\n",
    "    for key in GOVERNMENT_CODES:\n",
    "        if is_all_latin(key[0]):\n",
    "            gov_series.add(key[0])\n",
    "    return gov_series\n",
    "\n",
    "def get_region(region_code):\n",
    "    for region in REGION_CODES:\n",
    "        for code in REGION_CODES[region]:\n",
    "            if code == region_code:\n",
    "                return region\n",
    "    return \"NAN\"\n",
    "\n",
    "\n",
    "def get_plate_info(plate):\n",
    "    \"\"\"\n",
    "    The output will be: government_plate, region, first_three, last_three, full_number, series\n",
    "    \"\"\"\n",
    "    series1 = plate[0]\n",
    "    series2 = plate[4:6]\n",
    "    region_code = plate[6:]\n",
    "    gov_series = get_government_series()\n",
    "    series = series1 + series2\n",
    "    government_plate = 1 if (series) in gov_series else 0\n",
    "    region = get_region(region_code)\n",
    "    first_three = plate[1:4]\n",
    "    last_three = plate[6:9] if len(plate) > 8 else plate[6:8]\n",
    "    full_number = first_three + last_three\n",
    "    series = series\n",
    "    return government_plate, region, first_three, last_three, full_number, series\n",
    "\n",
    "print(\"Extracting features from the plate...\")\n",
    "df[['government_plate', 'region', 'first_three', 'last_three', 'full_number', 'series']] = \\\n",
    "    df['plate'].apply(get_plate_info).apply(pd.Series)\n",
    "print(\"Extracted: government_plate, region, first_three, last_three, full_number, series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted date features: year, month, day, day_of_week, weekend, week_of_year, quarter, day_name\n"
     ]
    }
   ],
   "source": [
    "# Extract the features from the date\n",
    "def extract_date_features(df):\n",
    "    '''\n",
    "    Extract the features from the date and also cycle the features\n",
    "    '''\n",
    "    # Basic date features\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df['date'].dt.quarter   \n",
    "    df['day_name'] = df['date'].dt.day_name()\n",
    "\n",
    "    # Cycle the features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)  \n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = extract_date_features(df)\n",
    "print(\"Extracted date features: year, month, day, day_of_week, weekend, week_of_year, quarter, day_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting government information...\n",
      "Extracted government information: government_agency, forbidden_to_buy, road_advantage, significance_level\n"
     ]
    }
   ],
   "source": [
    "# Get supplemental information from the given file\n",
    "df['government_agency'] = None\n",
    "df['forbidden_to_buy'] = False\n",
    "df['road_advantage'] = False\n",
    "df['significance_level'] = 0\n",
    "\n",
    "def get_government_info(row):\n",
    "    \"\"\"\n",
    "    Get the government information from the given row\n",
    "    \"\"\"\n",
    "    series = row['series']\n",
    "    number = int(row['first_three'])\n",
    "    region_code = row['last_three']\n",
    "    for (govern_series, (start, end), govern_region_code) in GOVERNMENT_CODES.keys():\n",
    "        if series == govern_series and start <= number <= end and region_code == govern_region_code:\n",
    "            agency = GOVERNMENT_CODES[(govern_series, (start, end), govern_region_code)][0]\n",
    "            forbiden = bool(GOVERNMENT_CODES[(govern_series, (start, end), govern_region_code)][1])\n",
    "            road_adv = bool(GOVERNMENT_CODES[(govern_series, (start, end), govern_region_code)][2])\n",
    "            significance = int(GOVERNMENT_CODES[(govern_series, (start, end), govern_region_code)][3])\n",
    "            return agency, forbiden, road_adv, significance\n",
    "    return None, False, False, 0\n",
    "\n",
    "print(\"Extracting government information...\")\n",
    "gov_info_df = df.apply(get_government_info, axis=1)\n",
    "df['government_agency'] = [info[0] for info in gov_info_df]\n",
    "df['forbidden_to_buy'] = [info[1] for info in gov_info_df]\n",
    "df['road_advantage'] = [info[2] for info in gov_info_df]\n",
    "df['significance_level'] = [info[3] for info in gov_info_df]\n",
    "print(\"Extracted government information: government_agency, forbidden_to_buy, road_advantage, significance_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the agency is not in the list, it is Non-Government\n",
      "Categorized government agency names\n",
      "Created binary variables for government agency using one-hot encoding\n"
     ]
    }
   ],
   "source": [
    "# Handle agency names\n",
    "df['government_agency'] = df['government_agency'].fillna('Non-Government')\n",
    "print('If the agency is not in the list, it is Non-Government')\n",
    "\n",
    "# Function to categorize the government agency\n",
    "def categorize_agency(agency):\n",
    "    if agency == 'Non-Government':\n",
    "        return 'Non-Government'\n",
    "    elif 'president' in agency.lower():\n",
    "        return 'Presidential'\n",
    "    elif 'police' in agency.lower() or 'internal affairs' in agency.lower():\n",
    "        return 'Police/Security'\n",
    "    elif 'government' in agency.lower():\n",
    "        return 'Government'\n",
    "    elif 'military' in agency.lower() or 'army' in agency.lower() or 'defense' in agency.lower():\n",
    "        return 'Military'\n",
    "    elif 'federal' in agency.lower():\n",
    "        return 'Federal Services'\n",
    "    elif 'judge' in agency.lower() or 'court' in agency.lower() or 'justice' in agency.lower() or 'prosecutor' in agency.lower():\n",
    "        return 'Judicial'\n",
    "    elif 'administration' in agency.lower():\n",
    "        return 'Administration'\n",
    "    else:\n",
    "        return 'Other Governmental'\n",
    "    \n",
    "df['agency_category'] = df['government_agency'].apply(categorize_agency)\n",
    "print(\"Categorized government agency names\")\n",
    "\n",
    "# Create binary variables for the government agency using one-hot encoding\n",
    "agency_dummies = pd.get_dummies(df['agency_category'], prefix='agency')\n",
    "df = pd.concat([df, agency_dummies], axis=1)\n",
    "print(\"Created binary variables for government agency using one-hot encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'has_repeated_letters' feature.\n",
      "Created 'has_sequential_numbers' feature.\n",
      "Created 'has_mirror_numbers' feature.\n",
      "Created 'is_beautiful_series' feature based on prestigious letter combinations.\n",
      "Created 'is_prestigious_number' feature based on specific prestigious number patterns.\n",
      "Created 'duplicated_two_last_numbers' feature based on specific last number patterns.\n",
      "Calculated 'letter_complexity' feature.\n",
      "Created 'has_repeated_numbers_full_number' feature.\n",
      "Calculated 'longest_run_length' feature.\n",
      "Created 'has_sequential_numbers_full_number' feature.\n",
      "Calculated 'prestige_score' by combining various prestige indicators.\n"
     ]
    }
   ],
   "source": [
    "# Check for repeated letters in the 'series' (e.g., 'AAA', 'XXX')\n",
    "# This feature indicates patterns that might be considered desirable.\n",
    "df['has_repeated_letters'] = df['series'].apply(lambda x: len(set(x)) == 1)\n",
    "print(\"Created 'has_repeated_letters' feature.\")\n",
    "\n",
    "# Check for repeated digits in the 'numbers' (e.g., '111', '777')\n",
    "# These are often considered \"beautiful\" or \"prestigious\" numbers.\n",
    "df['has_repeated_numbers'] = df['first_three'].str.replace(r'(.)(?=.*\\1)', '', regex=True).str.len() < df['first_three'].str.len()\n",
    "\n",
    "\n",
    "# Check for sequential digits (e.g., '123', '987')\n",
    "# Another pattern that can indicate prestige.\n",
    "consecutive_numbers = [\"123\", \"234\", \"345\", \"456\", \"567\", \"789\", \"987\", \"876\", \"765\", \"654\", \"543\", \"432\", \"321\"]\n",
    "df['has_sequential_numbers'] = df['first_three'].isin(consecutive_numbers)\n",
    "print(\"Created 'has_sequential_numbers' feature.\")\n",
    "\n",
    "# Check for mirror digits (e.g., '121', '303') or palindromic numbers (e.g., '111')\n",
    "# These are also considered special patterns.\n",
    "df['has_mirror_numbers'] = df['first_three'].apply(lambda x: x == x[::-1])\n",
    "print(\"Created 'has_mirror_numbers' feature.\")\n",
    "\n",
    "# Define a list of prestigious letter series (e.g., specific combinations like 'AAA', 'XXX')\n",
    "prestigious_letter_series = [\"AAA\", \"MMM\", \"EEE\", \"KKK\", \"OOO\", \"PPP\", \"CCC\", \"TTT\", \"XXX\", \"BBB\", \"YYY\", \"HHH\"]\n",
    "df['is_beautiful_series'] = df['series'].isin(prestigious_letter_series)\n",
    "print(\"Created 'is_beautiful_series' feature based on prestigious letter combinations.\")\n",
    "\n",
    "# Define a list of prestigious number combinations (e.g., single digits, triple digits, hundreds)\n",
    "prestigious_numbers = [\"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"009\", \"777\",\n",
    "                   \"010\", \"020\", \"030\", \"040\", \"050\", \"060\", \"070\", \"077\", \"080\", \"090\", \"707\",\n",
    "                   \"100\", \"111\", \"200\", \"222\", \"300\", \"333\", \"400\", \"444\", \"500\", \"555\", \"600\", \n",
    "                   \"666\", \"700\", \"800\", \"888\", \"900\", \"999\"] \n",
    "df['is_prestigious_number'] = df['first_three'].isin(prestigious_numbers)\n",
    "print(\"Created 'is_prestigious_number' feature based on specific prestigious number patterns.\")\n",
    "\n",
    "# Check if the last threee digits are 77 or 777\n",
    "special_last_numbers = [\"77\", \"777\"]\n",
    "df[\"duplicated_two_last_numbers\"] = df['last_three'].isin(special_last_numbers)\n",
    "print(\"Created 'duplicated_two_last_numbers' feature based on specific last number patterns.\")\n",
    "\n",
    "# Calculate the complexity of letters based on the number of unique characters\n",
    "# A lower complexity (e.g., 'AAA') might indicate simplicity and prestige.\n",
    "df['letter_complexity'] = df['series'].apply(\n",
    "    lambda x: len(set(x)) if pd.notnull(x) else 0\n",
    ")\n",
    "print(\"Calculated 'letter_complexity' feature.\")\n",
    "\n",
    "# Check for the whole number, eg. '777777', '123456'\n",
    "df['has_repeated_numbers_full_number'] = df['full_number'].apply(lambda x: len(set(x)) == 1)\n",
    "print(\"Created 'has_repeated_numbers_full_number' feature.\")\n",
    "\n",
    "# A column for the longest sequence of repeated characters in the plate\n",
    "def longest_run_length_str(d):\n",
    "    digits = np.array(list(d), dtype=int)  \n",
    "    change_points = np.where(np.diff(digits) != 0)[0] + 1\n",
    "    run_starts = np.concatenate(([0], change_points))\n",
    "    run_ends = np.concatenate((change_points, [len(digits)]))\n",
    "    run_lengths = run_ends - run_starts\n",
    "    return run_lengths.max()\n",
    "df['longest_run_length'] = df['full_number'].apply(longest_run_length_str)\n",
    "print(\"Calculated 'longest_run_length' feature.\")\n",
    "\n",
    "# Check for the monotone of the  whole number\n",
    "full_consecutive_numbers = [\"01234\", \"012345\", \"12345\", \"123456\", \"23456\", \"234567\", \"34567\", \"345678\", \"45678\", \"456789\", \"56789\",\n",
    "                            \"43210\", \"543210\", \"54321\", \"654321\", \"65432\", \"765432\", \"76543\", \"876543\", \"87654\", \"987654\", \"98765\"]\n",
    "df['has_sequential_numbers_full_number'] = df['full_number'].isin(full_consecutive_numbers)\n",
    "print(\"Created 'has_sequential_numbers_full_number' feature.\")\n",
    "\n",
    "# Create an overall prestige score by weighting different prestige-related features\n",
    "# This combines multiple signals into a single numeric score.\n",
    "df['prestige_score'] = (\n",
    "    (df['is_beautiful_series'].astype(int) * 3) + # Higher weight for beautiful letter series\n",
    "    (df['is_prestigious_number'].astype(int) * 2) + # Moderate weight for prestigious numbers\n",
    "    (df['has_repeated_letters'].astype(int) * 1) +\n",
    "    (df['has_repeated_numbers'].astype(int) * 1) +\n",
    "    (df['has_sequential_numbers'].astype(int) * 1) +\n",
    "    (df['has_mirror_numbers'].astype(int) * 1) +\n",
    "    (df['significance_level'].fillna(0)) # Include governmental significance level\n",
    ")\n",
    "print(\"Calculated 'prestige_score' by combining various prestige indicators.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Frequency Encoding to 'numbers' feature.\n",
      "Target encoded 'region_name' with 'region_mean_price'.\n",
      "Applied logarithmic transformation (log1p) to 'price' to create 'log_price'.\n"
     ]
    }
   ],
   "source": [
    "# Frequency Encoding for the 'numbers' feature\n",
    "# This replaces the number with its frequency of occurrence in the dataset.\n",
    "freq_table = df['first_three'].value_counts().reset_index()\n",
    "freq_table.columns = ['first_three', 'n']\n",
    "freq_table['freq_enc'] = freq_table['n'] / freq_table['n'].sum()\n",
    "freq_table['log_freq_enc'] = np.log1p(freq_table['freq_enc']) # Log transform for potential skewed distribution\n",
    "\n",
    "# Merge frequency encodings back to the main DataFrame\n",
    "df = df.merge(freq_table[['first_three', 'freq_enc', 'log_freq_enc']], \n",
    "              on='first_three', how='left')\n",
    "print(\"Applied Frequency Encoding to 'numbers' feature.\")\n",
    "\n",
    "# Target Encoding (Mean Encoding) for categorical features\n",
    "# This technique replaces a categorical value with the mean of the target variable\n",
    "# for that category. It's crucial to perform this only on the training data\n",
    "# to avoid data leakage from the test set.\n",
    "train_data = df[df['train'] == 1].copy()\n",
    "\n",
    "# For regions: calculate mean price for each region name\n",
    "region_mean_price = train_data.groupby('region')['price'].mean().reset_index()\n",
    "region_mean_price['log_region_mean_price'] = np.log1p(region_mean_price['price'])\n",
    "region_mean_price.drop(columns=['price'], inplace=True)\n",
    "df = df.merge(region_mean_price, on='region', how='left')\n",
    "print(\"Target encoded 'region_name' with 'region_mean_price'.\")\n",
    "\n",
    "# Logarithmic transformation of the target variable 'price'\n",
    "# This is a common practice in regression to make the target distribution more normal\n",
    "# and reduce the impact of outliers, improving model performance.\n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "print(\"Applied logarithmic transformation (log1p) to 'price' to create 'log_price'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'prestige_rank' based on 'prestige_score'.\n",
      "Added 'letter_number_combo' and 'is_gov_and_prestige' interaction features.\n",
      "Created 'is_premium_region' feature for major economic centers.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "df['prestige_rank'] = rankdata(df['prestige_score'].astype(int), method='average') / len(df)\n",
    "print(\"Created 'prestige_rank' based on 'prestige_score'.\")\n",
    "\n",
    "# Interaction Features:\n",
    "df['letter_number_combo'] = df['series'] + \"_\" + df['first_three'].astype(str)\n",
    "# Interaction between 'is_government' and 'prestige_score'\n",
    "df['is_gov_and_prestige'] = df['government_plate'] * df['prestige_score'].astype(int)\n",
    "print(\"Added 'letter_number_combo' and 'is_gov_and_prestige' interaction features.\")\n",
    "\n",
    "# Flag common premium regions (e.g., major cities/oblasts) as a binary feature.\n",
    "premium_regions = ['Moscow', 'Saint Petersburg', 'Moscow Oblast']\n",
    "df['is_premium_region'] = df['region'].isin(premium_regions).astype(int)\n",
    "print(\"Created 'is_premium_region' feature for major economic centers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone # For cloning estimators in cross-validation\n",
    "from sklearn.compose import ColumnTransformer # To apply different transformers to different columns\n",
    "from sklearn.pipeline import Pipeline # To chain multiple processing steps and a final estimator\n",
    "from sklearn.preprocessing import OrdinalEncoder, KBinsDiscretizer, OneHotEncoder # Various encoding/discretization methods\n",
    "from sklearn.model_selection import StratifiedKFold # Cross-validation strategy\n",
    "from xgboost import XGBRegressor # Gradient Boosting Machine from XGBoost\n",
    "from lightgbm import LGBMRegressor # Gradient Boosting Machine from LightGBM\n",
    "from catboost import CatBoostRegressor # Gradient Boosting Machine from CatBoost\n",
    "import category_encoders as ce # Advanced categorical encoders (install with: pip install category-encoders)\n",
    " \n",
    "seed = 42 # Random seed for reproducibility\n",
    "n_splits = 10\n",
    "\n",
    "target = 'log_price'\n",
    "drop_cols = ['id', 'train', 'plate', 'price', 'log_price', 'date', 'government_agency', 'agency_category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets.\n",
      "Training features (X) shape: (51635, 50)\n",
      "Training target (y) shape: (51635,)\n",
      "Test features (X_test) shape: (7695, 50)\n"
     ]
    }
   ],
   "source": [
    "# Separating the concatenated DataFrame back into original training and testing sets\n",
    "# based on the 'is_train' flag.\n",
    "train_df = df[df['train'] == 1].copy() # .copy() to avoid SettingWithCopyWarning\n",
    "test_df = df[df['train'] == 0].copy()\n",
    "\n",
    "# Defining the features (X) and the target (y) for the training set,\n",
    "# and features for the test set (X_test).\n",
    "# 'errors='ignore'' handles cases where a column in DROP_COLS might not exist, preventing errors.\n",
    "X = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "y = train_df[target].copy()\n",
    "X_test = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "print(\"Data split into training and testing sets.\")\n",
    "print(f\"Training features (X) shape: {X.shape}\")\n",
    "print(f\"Training target (y) shape: {y.shape}\")\n",
    "print(f\"Test features (X_test) shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Summary ➜ Numerical: 24 | Boolean: 19 | Low Cardinality Categorical: 1 | High Cardinality Categorical: 6\n"
     ]
    }
   ],
   "source": [
    "# This section is crucial for handling different data types dynamically.\n",
    "# It automatically identifies numerical, boolean, and categorical columns,\n",
    "# and further segments categorical columns by their cardinality to apply\n",
    "# appropriate encoding strategies.\n",
    "\n",
    "def detect_columns(X):\n",
    "    \"\"\"\n",
    "    Detects and segments columns by their data type and cardinality.\n",
    "    This helps in applying specific preprocessing steps to different column types.\n",
    "    \"\"\"\n",
    "    bool_cols = [c for c in X.columns if X[c].dtype == 'bool'] # Identify boolean columns\n",
    "    num_cols = [c for c in X.columns if X[c].dtype.kind in 'if' and c not in bool_cols] # Identify numerical (int/float) columns, excluding booleans\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols + bool_cols]  # Remaining columns are treated as categorical\n",
    "\n",
    "    # Further segmentation of categorical columns by cardinality (number of unique values)\n",
    "    # Different encoding strategies are optimal for different cardinalities.\n",
    "    cat_low = [c for c in cat_cols if X[c].nunique() <= 20] # Low cardinality for One-Hot Encoding\n",
    "    cat_high = [c for c in cat_cols if X[c].nunique() >= 20] # High cardinality for Target Encoding\n",
    "\n",
    "    print('\\nColumn Summary ➜ Numerical:', len(num_cols),\n",
    "          '| Boolean:', len(bool_cols),\n",
    "          '| Low Cardinality Categorical:', len(cat_low),\n",
    "          '| High Cardinality Categorical:', len(cat_high))\n",
    "\n",
    "    return num_cols, bool_cols, cat_low, cat_high\n",
    "\n",
    "# Apply the column detection function to the training features\n",
    "num_cols, bool_cols, cat_low, cat_high = detect_columns(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing pipeline (ColumnTransformer) defined.\n"
     ]
    }
   ],
   "source": [
    "# The `ColumnTransformer` is the core component here. It allows applying\n",
    "# different transformations to different subsets of columns in parallel.\n",
    "# This ensures that each column type is handled appropriately before feeding to the model.\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', num_cols), # Numerical columns: 'passthrough' means no transformation\n",
    "        ('bool', 'passthrough', bool_cols), # Boolean columns: 'passthrough' as they are already binary\n",
    "        ('low', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_low),  # One-Hot Encoding for low cardinality categories\n",
    "                                                                                        # 'handle_unknown='ignore'' prevents errors if new categories appear in test set\n",
    "                                                                                        # 'sparse_output=False' returns a dense NumPy array\n",
    "        ('high', ce.TargetEncoder(cols=cat_high, smoothing=0.2), cat_high)  # Target Encoding for high cardinality categories\n",
    "                                                                            # Replaces category with mean of target. 'smoothing' helps prevent overfitting.\n",
    "    ],\n",
    "    remainder='drop',  # Drops any columns not explicitly specified in `transformers` (safer approach)\n",
    "    n_jobs=-1  # Utilizes all available CPU cores for parallel processing during transformation\n",
    ")\n",
    "print(\"\\nPreprocessing pipeline (ColumnTransformer) defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models and their optimized hyperparameters defined.\n",
      "Pipelines constructed: Preprocessing -> Model.\n"
     ]
    }
   ],
   "source": [
    "# Optimized Hyperparameters for various Gradient Boosting Regressors.\n",
    "# These parameters are typically found through hyperparameter optimization\n",
    "# techniques like GridSearchCV, RandomizedSearchCV, or more advanced tools like Optuna.\n",
    "\n",
    "# XGBoost Parameters\n",
    "xgb_params = {\n",
    "            'n_estimators': 1495, \n",
    "            'max_depth': 11, \n",
    "            'learning_rate': 0.01320706464652558, \n",
    "            'subsample': 0.6617047575164631, \n",
    "            'colsample_bytree': 0.48638458903836135, \n",
    "            'reg_alpha': 0.033824195612960836, \n",
    "            'reg_lambda': 0.23379853342154155, \n",
    "            'gamma': 0.0049614495111536695,\n",
    "            'objective': 'reg:tweedie',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': seed}\n",
    "\n",
    "# Dictionary of models to be trained. Easily extensible to include more models.\n",
    "# Uncomment LGBM and CatBoost to include them in the ensemble.\n",
    "models = {\n",
    "    'XGB': XGBRegressor(**xgb_params),\n",
    "    #'LGBM': LGBMRegressor(**lgb_params),\n",
    "    #'CatBoost': CatBoostRegressor(**cat_params)\n",
    "}\n",
    "print(\"\\nModels and their optimized hyperparameters defined.\")\n",
    "\n",
    "# Construct the full pipeline for each model: preprocessing + estimator\n",
    "# Each pipeline handles all necessary data transformations before training the model.\n",
    "pipelines = {name: Pipeline(steps=[('prep', preprocess), ('model', model)]) for name, model in models.items()}\n",
    "print(\"Pipelines constructed: Preprocessing -> Model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMAPE evaluation metric defined.\n"
     ]
    }
   ],
   "source": [
    "# The Symmetric Mean Absolute Percentage Error (SMAPE) is often used in forecasting\n",
    "# and is robust to zero values in the actuals. It's defined once to ensure consistency.\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    Formula: (1/n) * Sum(|y_true - y_pred| / ((|y_true| + |y_pred|) / 2)) * 100\n",
    "    This metric handles cases where y_true or y_pred (or both) are zero.\n",
    "    \"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Handle division by zero: if denominator is zero (i.e., both y_true and y_pred are zero),\n",
    "    # the corresponding term for SMAPE is defined as 0.0 to avoid NaN/Inf.\n",
    "    smape_term = np.zeros_like(diff, dtype=float)\n",
    "    non_zero_denom = denominator != 0 # Identify where denominator is not zero\n",
    "    smape_term[non_zero_denom] = diff[non_zero_denom] / denominator[non_zero_denom]\n",
    "    \n",
    "    return np.mean(smape_term) * 100\n",
    "\n",
    "print(\"\\nSMAPE evaluation metric defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-22 00:36:10,443] A new study created in memory with name: no-name-27573558-ea81-4b7b-83fe-bf932c9f1089\n",
      "[I 2025-05-22 00:36:35,408] Trial 0 finished with value: 36.747288095125796 and parameters: {'n_estimators': 1345, 'max_depth': 10, 'learning_rate': 0.01441296218544771, 'subsample': 0.6698589896520895, 'colsample_bytree': 0.41517728613056454, 'reg_alpha': 0.011843121686051368, 'reg_lambda': 0.21646763642367575, 'gamma': 0.004229066093403417}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:37:06,521] Trial 1 finished with value: 36.89740744183984 and parameters: {'n_estimators': 1425, 'max_depth': 11, 'learning_rate': 0.019360571338285216, 'subsample': 0.6558066957919729, 'colsample_bytree': 0.4076655411460617, 'reg_alpha': 0.035319171432527115, 'reg_lambda': 0.2702059829719194, 'gamma': 0.0032819884694872538}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:37:37,535] Trial 2 finished with value: 36.98753237491073 and parameters: {'n_estimators': 1480, 'max_depth': 13, 'learning_rate': 0.024309262492486747, 'subsample': 0.622344917091388, 'colsample_bytree': 0.499027308934983, 'reg_alpha': 0.025763433070293603, 'reg_lambda': 0.27287409189467793, 'gamma': 0.0031311680172989177}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:39:00,544] Trial 3 finished with value: 37.0109970622957 and parameters: {'n_estimators': 1370, 'max_depth': 14, 'learning_rate': 0.010284346393051959, 'subsample': 0.6392251740637644, 'colsample_bytree': 0.440925752139001, 'reg_alpha': 0.014398287224096484, 'reg_lambda': 0.27003886453050213, 'gamma': 0.0010928788578762414}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:39:37,165] Trial 4 finished with value: 36.78285728862461 and parameters: {'n_estimators': 1429, 'max_depth': 10, 'learning_rate': 0.010138736242948357, 'subsample': 0.6054162910809131, 'colsample_bytree': 0.4426010993386577, 'reg_alpha': 0.025019951143999852, 'reg_lambda': 0.24057322415528018, 'gamma': 0.002726665152602272}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:40:45,684] Trial 5 finished with value: 36.92919995833976 and parameters: {'n_estimators': 1419, 'max_depth': 14, 'learning_rate': 0.014332594085808802, 'subsample': 0.6834571246927232, 'colsample_bytree': 0.48512680202468567, 'reg_alpha': 0.020194831641738395, 'reg_lambda': 0.27342424633479834, 'gamma': 0.0013277380495840686}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:41:33,825] Trial 6 finished with value: 36.97073234062374 and parameters: {'n_estimators': 1392, 'max_depth': 11, 'learning_rate': 0.02269046829184141, 'subsample': 0.6539114873178268, 'colsample_bytree': 0.41813872841634436, 'reg_alpha': 0.04258482360816458, 'reg_lambda': 0.26541696348741917, 'gamma': 0.0012382172362668776}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:42:23,205] Trial 7 finished with value: 36.81893709006641 and parameters: {'n_estimators': 1340, 'max_depth': 11, 'learning_rate': 0.01425545941433944, 'subsample': 0.6368303023549711, 'colsample_bytree': 0.42425042112797295, 'reg_alpha': 0.04483974126756681, 'reg_lambda': 0.20129858718116456, 'gamma': 0.0016530911407186854}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:43:25,681] Trial 8 finished with value: 37.019063170535226 and parameters: {'n_estimators': 1357, 'max_depth': 14, 'learning_rate': 0.013583093642891812, 'subsample': 0.6565489754514174, 'colsample_bytree': 0.4319294639424398, 'reg_alpha': 0.04820585069753932, 'reg_lambda': 0.21766527438456607, 'gamma': 0.0017391247225760755}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:43:57,749] Trial 9 finished with value: 36.93741518312681 and parameters: {'n_estimators': 1446, 'max_depth': 10, 'learning_rate': 0.024279083115727317, 'subsample': 0.6472605607546279, 'colsample_bytree': 0.448384704113199, 'reg_alpha': 0.020454475988826767, 'reg_lambda': 0.3258333882939807, 'gamma': 0.002275470390748296}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:44:21,762] Trial 10 finished with value: 36.767570141601276 and parameters: {'n_estimators': 1328, 'max_depth': 12, 'learning_rate': 0.02906767322344552, 'subsample': 0.6957729343534869, 'colsample_bytree': 0.4663722658023501, 'reg_alpha': 0.01050421277691578, 'reg_lambda': 0.32980882079832685, 'gamma': 0.004963500185820421}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:44:47,406] Trial 11 finished with value: 36.86185896998077 and parameters: {'n_estimators': 1306, 'max_depth': 12, 'learning_rate': 0.02988621209679956, 'subsample': 0.6995813360918709, 'colsample_bytree': 0.46522586953509804, 'reg_alpha': 0.01012967476610854, 'reg_lambda': 0.3482398827269619, 'gamma': 0.004727144563380864}. Best is trial 0 with value: 36.747288095125796.\n",
      "[I 2025-05-22 00:45:27,301] Trial 12 finished with value: 36.73042774056752 and parameters: {'n_estimators': 1308, 'max_depth': 12, 'learning_rate': 0.01732846863994011, 'subsample': 0.6830554354317933, 'colsample_bytree': 0.47060774793163646, 'reg_alpha': 0.010003816896893683, 'reg_lambda': 0.3121350555263032, 'gamma': 0.0046340694112464555}. Best is trial 12 with value: 36.73042774056752.\n",
      "[I 2025-05-22 00:46:05,669] Trial 13 finished with value: 36.85173583154445 and parameters: {'n_estimators': 1300, 'max_depth': 13, 'learning_rate': 0.01676513380046552, 'subsample': 0.6742822775955574, 'colsample_bytree': 0.4660717524675598, 'reg_alpha': 0.0139822427479616, 'reg_lambda': 0.3030303306156484, 'gamma': 0.003987349261876126}. Best is trial 12 with value: 36.73042774056752.\n",
      "[I 2025-05-22 00:46:36,003] Trial 14 finished with value: 36.80788558776549 and parameters: {'n_estimators': 1329, 'max_depth': 10, 'learning_rate': 0.017085738569935068, 'subsample': 0.672805916726841, 'colsample_bytree': 0.4060647928362591, 'reg_alpha': 0.013855382645335073, 'reg_lambda': 0.3081668955008731, 'gamma': 0.0037391688106790145}. Best is trial 12 with value: 36.73042774056752.\n",
      "[I 2025-05-22 00:47:19,419] Trial 15 finished with value: 36.78535346123659 and parameters: {'n_estimators': 1378, 'max_depth': 13, 'learning_rate': 0.01250320904886713, 'subsample': 0.6699222512373889, 'colsample_bytree': 0.4821378055875911, 'reg_alpha': 0.011841807538155428, 'reg_lambda': 0.2998519053270152, 'gamma': 0.0041075270805192585}. Best is trial 12 with value: 36.73042774056752.\n",
      "[I 2025-05-22 00:47:53,361] Trial 16 finished with value: 36.74736215421228 and parameters: {'n_estimators': 1350, 'max_depth': 11, 'learning_rate': 0.019739586188038158, 'subsample': 0.6860531206686622, 'colsample_bytree': 0.4574149759154482, 'reg_alpha': 0.015778198803337376, 'reg_lambda': 0.24412115631819092, 'gamma': 0.0026267346968684236}. Best is trial 12 with value: 36.73042774056752.\n",
      "[I 2025-05-22 00:48:37,446] Trial 17 finished with value: 36.68744452230098 and parameters: {'n_estimators': 1323, 'max_depth': 12, 'learning_rate': 0.012007252698923292, 'subsample': 0.6671839328995314, 'colsample_bytree': 0.4807763978527264, 'reg_alpha': 0.017331865508852094, 'reg_lambda': 0.24179266018998458, 'gamma': 0.0034002567335811107}. Best is trial 17 with value: 36.68744452230098.\n",
      "[I 2025-05-22 00:49:20,828] Trial 18 finished with value: 36.68830302131664 and parameters: {'n_estimators': 1318, 'max_depth': 12, 'learning_rate': 0.011798343118448984, 'subsample': 0.685409378054945, 'colsample_bytree': 0.48097127522270516, 'reg_alpha': 0.01664455499939064, 'reg_lambda': 0.24555642752831322, 'gamma': 0.003309507649768975}. Best is trial 17 with value: 36.68744452230098.\n",
      "[I 2025-05-22 00:50:09,755] Trial 19 finished with value: 36.684376911537896 and parameters: {'n_estimators': 1493, 'max_depth': 12, 'learning_rate': 0.011731532487467835, 'subsample': 0.663569119260017, 'colsample_bytree': 0.4964471128737775, 'reg_alpha': 0.017258632934593145, 'reg_lambda': 0.2515888473499397, 'gamma': 0.003266699674463174}. Best is trial 19 with value: 36.684376911537896.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_estimators': 1493, 'max_depth': 12, 'learning_rate': 0.011731532487467835, 'subsample': 0.663569119260017, 'colsample_bytree': 0.4964471128737775, 'reg_alpha': 0.017258632934593145, 'reg_lambda': 0.2515888473499397, 'gamma': 0.003266699674463174}\n",
      "Best CV SMAPE: 36.684376911537896\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1300, 1500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 14),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.03, log=True),  \n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.7),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 0.05, log=True),          \n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.2, 0.35),\n",
    "        'gamma': trial.suggest_float('gamma', 0.001, 0.005, log=True),               \n",
    "        'tweedie_variance_power': 1.0869464555654937,\n",
    "        'objective': 'reg:tweedie',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': seed,\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "    pipeline = Pipeline(steps=[('prep', preprocess), ('model', model)])\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    smape_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        pipe = clone(pipeline)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        preds = pipe.predict(X_val)\n",
    "        score = smape(np.exp(y_val.values), np.exp(preds))\n",
    "        smape_scores.append(score)\n",
    "\n",
    "    return np.mean(smape_scores)  # minimize mean SMAPE\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20, n_jobs=1)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best CV SMAPE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target variable ('price') binned into 10 strata for stratification.\n",
      "\n",
      "===== CROSS-VALIDATION TRAINING =====\n",
      "\n",
      "Initiating training for model: XGB...\n",
      "  Fold 01/10\n",
      "  Fold 02/10\n",
      "  Fold 03/10\n",
      "  Fold 04/10\n",
      "  Fold 05/10\n",
      "  Fold 06/10\n",
      "  Fold 07/10\n",
      "  Fold 08/10\n",
      "  Fold 09/10\n",
      "  Fold 10/10\n",
      "⮕  Overall CV SMAPE for XGB: 36.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Get the target variable and bin it into strata for stratification\n",
    "y_bins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile') \\\n",
    "    .fit_transform(y.values.reshape(-1, 1)).astype(int).ravel()\n",
    "print(f\"\\nTarget variable ('price') binned into {y_bins.max() + 1} strata for stratification.\")\n",
    "\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "oof_preds = {name: np.zeros(len(y)) for name in models}\n",
    "test_preds = {name: np.zeros(len(X_test)) for name in models}\n",
    "feature_importances = {}\n",
    "\n",
    "print('\\n===== CROSS-VALIDATION TRAINING =====')\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\nInitiating training for model: {model_name}...\")\n",
    "    try:\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y_bins), 1):\n",
    "            print(f\"  Fold {fold:02d}/{n_splits}\")\n",
    "            X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "            pipeline.fit(X_tr, y_tr)\n",
    "            oof_preds[model_name][val_idx] = pipeline.predict(X_val)\n",
    "            test_preds[model_name] += pipeline.predict(X_test) / n_splits\n",
    "\n",
    "        # Calculate the SMAPE for the out-of-fold predictions\n",
    "        cv_smape = smape(np.exp(y), np.exp(oof_preds[model_name]))\n",
    "        print(f'⮕  Overall CV SMAPE for {model_name}: {cv_smape:.2f}%')\n",
    "\n",
    "        if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "            feature_importances[model_name] = pipeline['model'].feature_importances_\n",
    "        elif hasattr(pipeline['model'], 'get_booster'): \n",
    "            feature_importances[model_name] = pipeline['model'].get_booster().get_score(importance_type='weight') \n",
    "        else:\n",
    "            feature_importances[model_name] = None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training of model {model_name}: {str(e)}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['government_plate', 'region', 'first_three', 'last_three',\n",
       "       'full_number', 'series', 'year', 'month', 'day', 'day_of_week',\n",
       "       'weekend', 'week_of_year', 'quarter', 'day_name', 'month_sin',\n",
       "       'month_cos', 'day_sin', 'day_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
       "       'forbidden_to_buy', 'road_advantage', 'significance_level',\n",
       "       'agency_Administration', 'agency_Federal Services', 'agency_Government',\n",
       "       'agency_Judicial', 'agency_Non-Government', 'agency_Other Governmental',\n",
       "       'agency_Police/Security', 'agency_Presidential', 'has_repeated_letters',\n",
       "       'has_repeated_numbers', 'has_sequential_numbers', 'has_mirror_numbers',\n",
       "       'is_beautiful_series', 'is_prestigious_number',\n",
       "       'duplicated_two_last_numbers', 'letter_complexity',\n",
       "       'has_repeated_numbers_full_number', 'longest_run_length',\n",
       "       'has_sequential_numbers_full_number', 'prestige_score', 'freq_enc',\n",
       "       'log_freq_enc', 'log_region_mean_price', 'prestige_rank',\n",
       "       'letter_number_combo', 'is_gov_and_prestige', 'is_premium_region'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               feature  importance\n",
      "21                       prestige_rank    0.095047\n",
      "39               is_prestigious_number    0.079891\n",
      "41    has_repeated_numbers_full_number    0.070085\n",
      "17                      prestige_score    0.066061\n",
      "25                      road_advantage    0.059778\n",
      "22                 is_gov_and_prestige    0.049516\n",
      "52                          last_three    0.043619\n",
      "40         duplicated_two_last_numbers    0.035820\n",
      "34                has_repeated_letters    0.035089\n",
      "53                         full_number    0.033891\n",
      "38                 is_beautiful_series    0.032571\n",
      "1                                 year    0.028828\n",
      "0                     government_plate    0.025732\n",
      "51                         first_three    0.021461\n",
      "28                   agency_Government    0.018033\n",
      "15                   letter_complexity    0.018004\n",
      "54                              series    0.017869\n",
      "14                  significance_level    0.015788\n",
      "30               agency_Non-Government    0.015324\n",
      "19                        log_freq_enc    0.014638\n",
      "18                            freq_enc    0.013846\n",
      "55                 letter_number_combo    0.011188\n",
      "16                  longest_run_length    0.010476\n",
      "32              agency_Police/Security    0.010316\n",
      "23                   is_premium_region    0.009467\n",
      "20                   region_mean_price    0.008931\n",
      "37                  has_mirror_numbers    0.008778\n",
      "29                     agency_Judicial    0.007833\n",
      "50                              region    0.007692\n",
      "35                has_repeated_numbers    0.006969\n",
      "27             agency_Federal Services    0.006947\n",
      "24                    forbidden_to_buy    0.006062\n",
      "7                              quarter    0.005758\n",
      "33                 agency_Presidential    0.005658\n",
      "6                         week_of_year    0.005410\n",
      "26               agency_Administration    0.005408\n",
      "36              has_sequential_numbers    0.005366\n",
      "9                            month_cos    0.005107\n",
      "47                   day_name_Thursday    0.005033\n",
      "48                    day_name_Tuesday    0.004959\n",
      "10                             day_sin    0.004904\n",
      "13                     day_of_week_cos    0.004854\n",
      "8                            month_sin    0.004837\n",
      "3                                  day    0.004805\n",
      "11                             day_cos    0.004705\n",
      "46                     day_name_Sunday    0.004694\n",
      "2                                month    0.004620\n",
      "43                     day_name_Friday    0.004574\n",
      "44                     day_name_Monday    0.004564\n",
      "49                  day_name_Wednesday    0.004556\n",
      "12                     day_of_week_sin    0.004482\n",
      "45                   day_name_Saturday    0.004438\n",
      "4                          day_of_week    0.004414\n",
      "5                              weekend    0.004413\n",
      "31           agency_Other Governmental    0.003680\n",
      "42  has_sequential_numbers_full_number    0.003212\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names(preprocessor):\n",
    "    output_features = []\n",
    "\n",
    "    # Add numeric and bool columns (passthrough)\n",
    "    output_features.extend(num_cols)\n",
    "    output_features.extend(bool_cols)\n",
    "\n",
    "    # OneHotEncoder feature names\n",
    "    onehot = preprocessor.named_transformers_['low']\n",
    "    onehot_features = onehot.get_feature_names_out(cat_low).tolist()\n",
    "    output_features.extend(onehot_features)\n",
    "\n",
    "    # TargetEncoder columns (1 col per high cardinality cat)\n",
    "    output_features.extend(cat_high)\n",
    "\n",
    "    return output_features\n",
    "\n",
    "\n",
    "feature_names = get_feature_names(pipeline.named_steps['prep'])\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances['XGB']\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "print(feat_imp_df)  # Top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'price': np.exp(test_preds['XGB'])\n",
    "})\n",
    "submission.to_csv('submission8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
